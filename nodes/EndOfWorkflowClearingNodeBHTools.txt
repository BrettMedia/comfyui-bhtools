import gc
import os
import logging
import shutil
import tempfile
from pathlib import Path

import psutil
import torch

logger = logging.getLogger(__name__)

def clear_temp_dir(pattern="*comfy*") -> int:
    count = 0
    temp_dir = Path(tempfile.gettempdir())
    for p in temp_dir.glob(pattern):
        try:
            if p.is_file():
                p.unlink()
            else:
                shutil.rmtree(p)
            count += 1
        except Exception as e:
            logger.warning(f"Failed to remove {p}: {e}")
    return count

def unload_comfyui_models() -> bool:
    try:
        import comfy.model_management as mm
        if hasattr(mm, 'unload_all_models'):
            mm.unload_all_models()
            return True
        if hasattr(mm, 'cleanup_models'):
            mm.cleanup_models()
            return True
    except Exception as e:
        logger.warning(f"Model unload error: {e}")
    return False

def gpu_clear(method: str, reset_peak: bool = False) -> None:
    torch.cuda.empty_cache()
    if method in ("hard", "complete"):
        torch.cuda.ipc_collect()
        torch.cuda.synchronize()
    if method == "complete" and reset_peak:
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.reset_max_memory_allocated()
        torch.cuda.reset_max_memory_cached()

class EndOfWorkflowClearingNodeBHTools:
    """
    ComfyUI node: final cleanup at workflow end, returns only the status report.
    """
    def __init__(self):
        self._has_run = False

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "trigger": ("*", {}),
                "clear_vram": ("BOOLEAN", {"default": True}),
                "clear_models": ("BOOLEAN", {"default": True}),
                "clear_torch_cache": ("BOOLEAN", {"default": False}),
                "clear_system_cache": ("BOOLEAN", {"default": False}),
                "clear_temp_files": ("BOOLEAN", {"default": False}),
                "force_gc": ("BOOLEAN", {"default": True}),
                "vram_clear_method": (["soft", "hard", "complete"], {"default": "hard"}),
                "only_if_above_threshold": ("BOOLEAN", {"default": False}),
                "only_run_once": ("BOOLEAN", {"default": True}),
                "verbose": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "vram_threshold_gb": ("FLOAT", {
                    "default": 2.0, "min": 0.0, "max": 64.0, "step": 0.1
                }),
                "ram_threshold_gb": ("FLOAT", {
                    "default": 4.0, "min": 0.0, "max": 128.0, "step": 0.1
                }),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("cleanup_report",)
    FUNCTION = "end_of_workflow_cleanup"
    CATEGORY = "utils/memory"

    def end_of_workflow_cleanup(
        self,
        trigger,
        clear_vram,
        clear_models,
        clear_torch_cache,
        clear_system_cache,
        clear_temp_files,
        force_gc,
        vram_clear_method,
        only_if_above_threshold,
        only_run_once,
        verbose,
        vram_threshold_gb=2.0,
        ram_threshold_gb=4.0,
    ):
        # only run once guard
        if only_run_once and self._has_run:
            return ("Cleanup already executed, skipping.",)
        if only_run_once:
            self._has_run = True

        status = []

        # validate thresholds
        if vram_threshold_gb < 0 or ram_threshold_gb < 0:
            raise ValueError("Threshold values must be non-negative")

        # initial snapshot
        vm = psutil.virtual_memory()
        initial_ram = vm.used / 1024**3
        initial_vram = (
            torch.cuda.memory_allocated() / 1024**3
            if torch.cuda.is_available() else 0.0
        )

        if verbose:
            msg = f"Start â€” RAM: {initial_ram:.2f} GB"
            if torch.cuda.is_available():
                msg += f", VRAM: {initial_vram:.2f} GB"
            status.append(msg)

        # conditional skip
        if only_if_above_threshold:
            ram_cond = initial_ram >= ram_threshold_gb
            vram_cond = initial_vram >= vram_threshold_gb
            if not (ram_cond or vram_cond):
                status.append("âœ“ Usage below thresholds, cleanup skipped")
                return ("\n".join(status),)
            if ram_cond:
                status.append(f"âš  RAM â‰¥ {ram_threshold_gb:.1f} GB")
            if vram_cond:
                status.append(f"âš  VRAM â‰¥ {vram_threshold_gb:.1f} GB")

        # unload models
        if clear_models and torch.cuda.is_available():
            if unload_comfyui_models():
                status.append("âœ“ Models unloaded")
            else:
                status.append("âš  Model unload skipped")

        # garbage collection
        if force_gc:
            collected = gc.collect()
            status.append(f"âœ“ GC freed {collected} objects")

        # VRAM clear
        if clear_vram and torch.cuda.is_available():
            try:
                gpu_clear(vram_clear_method, reset_peak=(vram_clear_method == "complete"))
                status.append(f"âœ“ VRAM {vram_clear_method} clear")
            except Exception as e:
                status.append(f"âš  VRAM clear error: {e}")

        # torch cache
        if clear_torch_cache:
            try:
                torch.cuda.empty_cache()
                status.append("âœ“ PyTorch cache cleared")
            except Exception as e:
                status.append(f"âš  Torch cache error: {e}")

        # system cache
        if clear_system_cache:
            try:
                if os.name == "posix":
                    ret = os.system("sync")
                    status.append("âœ“ System sync" if ret == 0 else f"âš  Sync returned {ret}")
                else:
                    status.append("âœ“ Windows cache no-op")
            except Exception as e:
                status.append(f"âš  System cache error: {e}")

        # temp files
        if clear_temp_files:
            removed = clear_temp_dir()
            status.append(f"âœ“ Temp files removed: {removed}")

        # final snapshot
        if verbose:
            vm2 = psutil.virtual_memory()
            final_ram = vm2.used / 1024**3
            freed_ram = max(0.0, initial_ram - final_ram)
            status.append(f"End â€” RAM freed {freed_ram:.2f} GB")
            if torch.cuda.is_available():
                final_vram = torch.cuda.memory_allocated() / 1024**3
                freed_vram = max(0.0, initial_vram - final_vram)
                status.append(f"VRAM freed {freed_vram:.2f} GB")

        status.append("ðŸŽ‰ Cleanup complete!")
        report = "\n".join(status)
        return (report,)


NODE_CLASS_MAPPINGS = {
    "EndOfWorkflowClearingNodeBHTools": EndOfWorkflowClearingNodeBHTools,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "EndOfWorkflowClearingNodeBHTools": "ðŸŽ¬ End Workflow Cleanup | BH Tools",
}